{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versa API workstation setup\n",
    "# https://git.ucsf.edu/academic-research-systems/versa-api-workstation/\n",
    "\n",
    "# Mulesoft/Azure API test\n",
    "# https://git.ucsf.edu/academic-research-systems/azure-openai-demo/blob/master/test_mulesoft_azure_api.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"API_KEY\"] = \"your-API-key\"\n",
    "os.environ[\"API_VERSION\"]='2024-02-01'\n",
    "os.environ[\"RESOURCE_ENDPOINT\"]=\"https://unified-api.ucsf.edu/general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get('API_KEY')  # Match the environment variable name to the name you used in the .env file\n",
    "API_VERSION = os.environ.get('API_VERSION')\n",
    "RESOURCE_ENDPOINT = os.environ.get('RESOURCE_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add future/delete deprecated deployments to/from the lists below\n",
    "completions_deployments = ['text-davinci-003', 'gpt-35-turbo-instruct']\n",
    "embeddings_deployments = ['text-embedding-ada-002', ]\n",
    "chat_deployments = ['gpt-35-turbo',\n",
    "                    'gpt-35-turbo-0301',\n",
    "                    'gpt-4',\n",
    "                    'gpt-35-turbo-16K',\n",
    "                    'gpt-4-32K',\n",
    "                    'gpt-4-turbo',\n",
    "                    'gpt-4o-2024-05-13'\n",
    "                    ] # Deployment choice uniquely determines the underlying model\n",
    "\n",
    "\n",
    "def test_key():\n",
    "    assert API_KEY is not None and API_KEY.strip() != \"\", \"API Key is missing or empty\"\n",
    "    try:\n",
    "        redacted_key = API_KEY[0] + \"*\" * (len(API_KEY) - 3) + API_KEY[-2:]\n",
    "        base64.b64decode(API_KEY)\n",
    "        print(f\"API Key is a valid base64 string with length={len(API_KEY)}: {redacted_key}\")\n",
    "    except Exception as e:\n",
    "        assert False, f\"API Key is not a valid base64 string: {redacted_key} \" + str(e)\n",
    "\n",
    "\n",
    "def test_version():\n",
    "    assert API_VERSION is not None and API_VERSION.strip() != \"\", \"API Version is missing or empty\"\n",
    "    pattern = r'\\d{4}-\\d{2}-\\d{2}'  # matches four digits-two digits-two digits\n",
    "    assert re.fullmatch(pattern,\n",
    "                        API_VERSION) is not None, f\"API version has invalid format, it should be like: yyyy-mm-dd: {API_VERSION}\"\n",
    "    print(f\"API version has valid format: yyyy-mm-dd: {API_VERSION}\")\n",
    "\n",
    "\n",
    "def test_endpoint():\n",
    "    assert RESOURCE_ENDPOINT is not None and RESOURCE_ENDPOINT.strip() != \"\", \"Resource endpoint is missing or empty\"\n",
    "    url = urllib.parse.urlparse(RESOURCE_ENDPOINT)\n",
    "    assert all([url.scheme, url.netloc]), f\"Resource endpoint is not a valid URL: {RESOURCE_ENDPOINT}\"\n",
    "    print(f\"Resource endpoint is a valid URL: {RESOURCE_ENDPOINT}\")\n",
    "\n",
    "\n",
    "def test_completions():\n",
    "    for deployment_id in completions_deployments:\n",
    "        print(f\"\\nTesting completions for deployment: {deployment_id}\")\n",
    "        completions_url = f\"{RESOURCE_ENDPOINT}/openai/deployments/{deployment_id}/completions?api-version={API_VERSION}\"\n",
    "        prompt = 'The rain in Spain'\n",
    "        body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 30,  # Limit the response\n",
    "        })\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'api-key': API_KEY\n",
    "        }\n",
    "        response = requests.post(completions_url, headers=headers, data=body)\n",
    "        assert response.status_code == 200, f\"Test failed for deployment: {deployment_id}, Response status code: {response.status_code}, Response: {response.text}\"\n",
    "        print('User: ', prompt)\n",
    "        print('Response: ', json.loads(response.text).get('choices')[0].get('text'))\n",
    "\n",
    "\n",
    "def test_embeddings():\n",
    "    for deployment_id in embeddings_deployments:\n",
    "        print(f\"\\nTesting embeddings for deployment: {deployment_id}\")\n",
    "        embeddings_url = f\"{RESOURCE_ENDPOINT}/openai/deployments/{deployment_id}/embeddings?api-version={API_VERSION}\"\n",
    "        body = json.dumps({\n",
    "            \"input\": \"This is test string to embed\",\n",
    "        })\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'api-key': API_KEY\n",
    "        }\n",
    "        response = requests.post(embeddings_url, headers=headers, data=body)\n",
    "        assert response.status_code == 200, f\"Test failed for deployment: {deployment_id}, Response status code: {response.status_code}, Response: {response.text}\"\n",
    "        embedding_len = len(json.loads(response.text)['data'][0]['embedding'])\n",
    "        # print(embedding_len)\n",
    "\n",
    "        if deployment_id == 'text-embedding-ada-002':\n",
    "            assert_len = 1536\n",
    "        else:\n",
    "            raise ValueError(f'Deployment {deployment_id} not supported for validation. Check code')\n",
    "\n",
    "        assert embedding_len == assert_len, f\"Test failed for deployment: {deployment_id}, Response status code: {response.status_code}, Response: {response.text}\"\n",
    "\n",
    "        print('Embedding received from API')\n",
    "\n",
    "\n",
    "def test_chat_completions():\n",
    "    for deployment in chat_deployments:\n",
    "        print(f\"\\nTesting chat completions for deployment: {deployment}\")\n",
    "        url = f'{RESOURCE_ENDPOINT}/openai/deployments/{deployment}/chat/completions?api-version={API_VERSION}'\n",
    "        prompt = 'Hello, how are you today?'\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        })\n",
    "        headers = {'Content-Type': 'application/json', 'api-key': API_KEY}\n",
    "        response = requests.post(url, headers=headers, data=body)\n",
    "        print('User: ', prompt)\n",
    "        print('Response: ', json.loads(response.text).get('choices')[0].get('message').get('content'))\n",
    "        assert response.status_code == 200, f\"Test failed for deployment: {deployment}, model: {model}, Response status code: {response.status_code}, Response: {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute the tests\n",
    "# # First perform some basic validation of our environment variables\n",
    "# test_key()\n",
    "# test_version()\n",
    "# test_endpoint()\n",
    "\n",
    "# # Most API users will only need to use chat completions\n",
    "# test_chat_completions()  # Responds as an AI assistant\n",
    "# test_completions()  # Continues a thought or sentence in the prompt\n",
    "# test_embeddings()  # Validates a properly formed embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, model='gpt-4o-2024-05-13', temperature=0.1):\n",
    "    url = f'{RESOURCE_ENDPOINT}/openai/deployments/{model}/chat/completions?api-version={API_VERSION}'\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    })\n",
    "    headers = {'Content-Type': 'application/json', 'api-key': API_KEY}\n",
    "    response = requests.post(url, headers=headers, data=body)\n",
    "    return json.loads(response.text).get('choices')[0].get('message').get('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file with 2 columns: study_id, radiology_report_text\n",
    "df = pd.read_csv('/path/to/radiology_reports.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send to Versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_categories = [\"single_prompt\", \"single_prompt_with_explanation\"]\n",
    "prompt_prefixes = {\n",
    "    \"single_prompt\": \"Provide an AO/OTA classification label for the following fracture description. Be as precise as possible, including subgroups, universal modifiers, and qualifiers if available. Provide only the label in your response, with no explanation. If there are multiple fractures identified, provide a comma-separated list.\",\n",
    "    \"single_prompt_with_explanation\": \"Provide an AO/OTA classification label for the following fracture description. Be as precise as possible, including subgroups, universal modifiers, and qualifiers if available. Provide the label in your response, followed by a brief explanation for each part of the classification. If there are multiple fractures identified, provide a comma-separated list.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    for prompt in prompt_categories:\n",
    "        full_prompt = f\"{prompt_prefixes[prompt]}: {row['radiology_report_text']}\"\n",
    "        response = get_response(full_prompt)\n",
    "        results.append((row['study_id'], prompt, prompt_prefixes[prompt], row['radiology_report_text'], response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results, columns=[\"study_id\", \"prompt\", \"prompt_prefix\", \"radiology_report_text\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"/path/to/output_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
